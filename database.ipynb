{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducible experimental protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds the database with all the information we need to perform domain-adversarial speech activity detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python packages\n",
    "\n",
    "- pyannote.audio\n",
    "- pyannote.core\n",
    "- pyannote.database\n",
    "- pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `AMI`: [A multi-modal data set consisting of 100 hours of meeting recordings](http://groups.inf.ed.ac.uk/ami/corpus/)\n",
    "- `ldc2019e31`: [Second DIHARD Challenge Development Data](https://coml.lscp.ens.fr/dihard/)\n",
    "- `ldc2019e32`: [Second DIHARD Challenge Evaluation Data](https://coml.lscp.ens.fr/dihard/)\n",
    "- `musan`: [A corpus of MUsic, Speech, And Noise](https://www.openslr.org/17/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# where AMI has been downloaded from http://groups.inf.ed.ac.uk/ami/corpus/\n",
    "# Note that we'll just use the Mix-Headset subset for our experiments.\n",
    "ami = '/export/corpora4/ami/amicorpus'\n",
    "\n",
    "# where ldc2019e31 dataset has been downloaded\n",
    "ldc2019e31 = '/vol/corpora1/data/ldc/ldc2019e31/LDC2019E31_Second_DIHARD_Challenge_Development_Data'\n",
    "\n",
    "# where ldc2019e32 dataset has been downloaded \n",
    "ldc2019e32 = '/vol/corpora1/data/ldc/ldc2019e32/LDC2019E32_Second_DIHARD_Challenge_Evaluation_Data_V1.1'\n",
    "\n",
    "# where MUSAN has been downloaded from https://www.openslr.org/17/\n",
    "musan = '/vol/corpora4/musan'\n",
    "\n",
    "# where github.com/hbredin/DomainAdversarialVoiceActivityDetection has been cloned\n",
    "ROOT = '/vol/work1/bredin/jsalt/DomainAdversarialVoiceActivityDetection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# create 'database' sub-directory that is meant to store audio and reference files\n",
    "!mkdir -p {ROOT}/database/AMI\n",
    "!mkdir -p {ROOT}/database/DIHARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# define utility functions\n",
    "\n",
    "from pyannote.core import Timeline\n",
    "from pyannote.core import Annotation\n",
    "from typing import TextIO\n",
    "\n",
    "def write_rttm(file: TextIO, reference: Annotation):\n",
    "    \"\"\"Write reference annotation to \"rttm\" file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : file object\n",
    "    reference : `pyannote.core.Annotation`\n",
    "        Reference annotation\n",
    "    \"\"\"\n",
    "\n",
    "    for s, t, l in reference.itertracks(yield_label=True):\n",
    "        line = (\n",
    "            f'SPEAKER {reference.uri} 1 {s.start:.3f} {s.duration:.3f} '\n",
    "            f'<NA> <NA> {l} <NA> <NA>\\n'\n",
    "        )\n",
    "        file.write(line)\n",
    "\n",
    "def write_uem(file: TextIO, uem: Timeline):\n",
    "    \"\"\"Write evaluation map to \"uem\" file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : file object\n",
    "    uem : `pyannote.core.Timeline`\n",
    "        Evaluation timeline\n",
    "    \"\"\"\n",
    "\n",
    "    for s in uem:\n",
    "        line = f'{uem.uri} 1 {s.start:.3f} {s.end:.3f}\\n'\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we convert AMI annotations into the .rttm format. This piece of code will create a rttm folder in the AMI in {ROOT}/AMI folder :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os, glob\n",
    "import shutil\n",
    "\n",
    "def get_participants_per_meeting(annotations_folder):\n",
    "    \"\"\"\n",
    "    Parse corpusResources/meetings.xml to return a dictionnary whose keys are the meeting ids,\n",
    "    and values are a dictionnary containing (letter of the speaker, speaker id)\n",
    "    \"\"\"\n",
    "    root = ET.parse(os.path.join(annotations_folder,'corpusResources/meetings.xml')).getroot()\n",
    "\n",
    "    participants_per_meeting = {}\n",
    "    for m in root:          # meeting\n",
    "        m_id = m.attrib[\"observation\"]\n",
    "        speakers = {}\n",
    "        for s in m:         # speaker\n",
    "            speaker_id = s.attrib[\"global_name\"]\n",
    "            nxt_agent = s.attrib[\"nxt_agent\"]\n",
    "            speakers[nxt_agent] = speaker_id\n",
    "        participants_per_meeting[m_id] = speakers\n",
    "\n",
    "    return participants_per_meeting\n",
    "\n",
    "\n",
    "def ami_xml_to_rttm(annotations_folder, participants_per_meeting):\n",
    "    files = glob.iglob(os.path.join(annotations_folder, \"segments/*.xml\"))\n",
    "    rttm_dic = {}   # for storing the content of the rttm files (key = filename, value = content)\n",
    "    rttm_folder = os.path.join(ROOT, 'database', 'AMI', 'rttm')\n",
    "    shutil.rmtree(rttm_folder, ignore_errors=True)\n",
    "    os.makedirs(rttm_folder)\n",
    "    for f in files:\n",
    "        basename = os.path.basename(f).split(\".\")\n",
    "        meeting_id = basename[0]\n",
    "        speaker_letter = basename[1]\n",
    "        rttm_filepath = os.path.join(rttm_folder, meeting_id + \".rttm\", )\n",
    "\n",
    "        if rttm_filepath  not in rttm_dic.keys():\n",
    "            rttm_dic[rttm_filepath] = []\n",
    "\n",
    "        if speaker_letter in participants_per_meeting[meeting_id]:\n",
    "            speaker_id = participants_per_meeting[meeting_id][speaker_letter]\n",
    "        else:\n",
    "            print(\"Can't find which speaker is associated to letter %s in %s\" % (speaker_letter, f))\n",
    "\n",
    "        root = ET.parse(f).getroot()\n",
    "        for seg in root:\n",
    "            onset = float(seg.attrib[\"transcriber_start\"])\n",
    "            duration = float(seg.attrib[\"transcriber_end\"])-onset\n",
    "            rttm_dic[rttm_filepath].append([\"SPEAKER\", str(meeting_id), \"1\", \"%.6f\" % onset, \"%.6f\" % duration, \"<NA>\", \"<NA>\", speaker_id, \"1\\n\"])\n",
    "\n",
    "    # Sort dictionnary by onset\n",
    "    for k, v in rttm_dic.items():\n",
    "        rttm_dic[k] = sorted(v, key=lambda x: float(x[3]))\n",
    "\n",
    "    # Write rttm\n",
    "    for k, v in rttm_dic.items():\n",
    "        with open(k, 'a') as output_rttm:\n",
    "            for line in v:\n",
    "                output_rttm.write('\\t'.join(line))\n",
    "\n",
    "# Path to the annotations folder such as provided by the download\n",
    "annotations_folder = os.path.join(ami, \"annotations\")\n",
    "participants_per_meeting = get_participants_per_meeting(annotations_folder)\n",
    "ami_xml_to_rttm(annotations_folder, participants_per_meeting)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If everything went as expected, you should have the following outputs : "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Can't find which speaker is associated to letter D in /export/corpora4/ami/amicorpus/annotations/segments/EN2009b.D.segments.xml\n",
    "Can't find which speaker is associated to letter D in /export/corpora4/ami/amicorpus/annotations/segments/EN2003a.D.segments.xml\n",
    "Can't find which speaker is associated to letter D in /export/corpora4/ami/amicorpus/annotations/segments/IN1001.D.segments.xml\n",
    "Can't find which speaker is associated to letter D in /export/corpora4/ami/amicorpus/annotations/segments/EN2009c.D.segments.xml\n",
    "Can't find which speaker is associated to letter D in /export/corpora4/ami/amicorpus/annotations/segments/EN2002c.D.segments.xml"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%script false\n",
     "is_executing": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Those are the meetings for which there were only 3 speakers (instead of 4 usually).\n",
    "\n",
    "This corpora is originally provided without a train/dev/test split. However, we followed the \"Full-corpus partition\" proposed [here](http://groups.inf.ed.ac.uk/ami/corpus/datasets.shtml).\n",
    "You can use the same partition by following this piece of code :"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import shutil\n",
    "\n",
    "train = [\"ES2002\", \"ES2005\", \"ES2006\", \"ES2007\", \"ES2008\", \"ES2009\",\n",
    "         \"ES2010\", \"ES2012\", \"ES2013\", \"ES2015\", \"ES2016\", \"IS1000\",\n",
    "         \"IS1001\", \"IS1002\", \"IS1003\", \"IS1004\", \"IS1005\",\n",
    "         \"IS1006\", \"IS1007\", \"TS3005\", \"TS3008\", \"TS3009\", \"TS3010\", \"TS3011\",\n",
    "         \"TS3012\", \"EN2001\", \"EN2003\", \"EN2004a\", \"EN2005a\", \"EN2006\", \"EN2009\",\n",
    "         \"IN1001\", \"IN1002\", \"IN1005\", \"IN1007\", \"IN1008\", \"IN1009\", \"IN1012\", \"IN1013\",\n",
    "         \"IN1014\", \"IN1016\"]\n",
    "\n",
    "dev = [\"ES2003\", \"ES2011\", \"IS1008\", \"TS3004\", \"TS3006\",\n",
    "       \"IB4001\", \"IB4002\", \"IB4003\", \"IB4004\", \"IB4010\", \"IB4011\"]\n",
    "\n",
    "test = [\"ES2004\", \"ES2014\", \"IS1009\", \"TS3003\", \"TS3007\", \"EN2002\"]\n",
    "\n",
    "# Create needed folders\n",
    "AMI_folder = f'{ROOT}/database/AMI'\n",
    "\n",
    "for fold in [\"train\", \"dev\", \"test\"]:\n",
    "    shutil.rmtree(\"%s/%s\" % (AMI_folder, fold), ignore_errors=True)\n",
    "    os.makedirs(\"%s/%s\" % (AMI_folder, fold))\n",
    "    os.makedirs(\"%s/%s/gold\" % (AMI_folder, fold))\n",
    "    os.makedirs(\"%s/%s/wav\" % (AMI_folder, fold))\n",
    "\n",
    "# Get all the rttm files in the right location\n",
    "all = [train, dev, test]\n",
    "fold_names = [\"train\", \"dev\", \"test\"]\n",
    "for i in range(0, len(fold_names)):\n",
    "    fold_meeting_ids = all[i]\n",
    "    fold_name = fold_names[i]\n",
    "    for m_id in fold_meeting_ids:\n",
    "        files = glob.glob(os.path.join(AMI_folder, 'rttm', '%s*.rttm' % m_id))\n",
    "        if len(files) == 0:\n",
    "            print(\"Can't find files whose name matches with the regular experession %s*.rttm\" % m_id)\n",
    "            print(\"Something bad happened. You should consider restarting from the beginning.\")\n",
    "        for rttm_file in files:\n",
    "            shutil.copyfile(rttm_file, \"%s/%s/gold/%s\" % (AMI_folder, fold_name, os.path.basename(rttm_file).replace('.rttm', '.Mix-Headset.rttm')))\n",
    "print(\"Done.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a train/dev/test folder, themselves containing a gold and a wav folder, we can bring the audios in the latter by creating symbolic links :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import shutil\n",
    "\n",
    "folds = [\"train\", \"dev\", \"test\"]\n",
    "for fold in folds:\n",
    "    files = glob.iglob(\"%s/database/AMI/%s/gold/*.Mix-Headset.rttm\" % (ROOT, fold))\n",
    "    print(\"%s/AMI/%s/gold/*.Mix-Headset.rttm\" % (ROOT, fold))\n",
    "    for rttm in files:\n",
    "        basename = os.path.basename(rttm)           #EN2001a.Mix-Headset.rttm\n",
    "        meeting_id = basename.split('.')[0]         #EN2001a\n",
    "        filename = basename.replace(\".rttm\", \"\")    #EN2001a.Mix-Headset\n",
    "\n",
    "        wav = os.path.join(\"%s/%s/audio/%s.wav\" % (ami, meeting_id, filename))\n",
    "\n",
    "        # Symlink to the .wav file\n",
    "        os.symlink(wav, \"%s/database/AMI/%s/wav/%s.wav\" % (ROOT, fold, filename))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can create four files per (domain, subset) pair:\n",
    "- `{domain}.{subset}.txt` contains list of files\n",
    "- `{domain}.{subset}.rttm` contains manual annotation\n",
    "- `{domain}.{subset}.uem` contains unpartitioned evaluation map (uem)\n",
    "- `{domain}.domain.{subset}.txt` contains file-to-domain mapping\n",
    "\n",
    "#TO DO TO DO TO DO\n",
    "\n",
    "## DIHARD"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, development and evaluation subsets share the same names: `DH_0001` to `DH_0192` exist in both subsets.  \n",
    "To avoid any confusion in `pyannote.database`, we create symbolic links so we can distinguish `dev/DH_0001` from `tst/DH_0001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "ln: impossible de créer le lien symbolique '/home/lavechin/Bureau/DomainAdversarialVoiceActivityDetection/database/DIHARD/dev/flac': Le fichier existe\r\n",
      "ln: impossible de créer le lien symbolique '/home/lavechin/Bureau/DomainAdversarialVoiceActivityDetection/database/DIHARD/tst/flac': Le fichier existe\r\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "!ln --symbolic {ldc2019e31}/data/single_channel/flac {ROOT}/database/DIHARD/dev\n",
    "!ln --symbolic {ldc2019e32}/data/single_channel/flac {ROOT}/database/DIHARD/tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "# load list of test files (and their domain)\n",
    "\n",
    "tst = read_csv(f'{ldc2019e32}/docs/sources.tbl', \n",
    "               delim_whitespace=True,\n",
    "               names=['uri', 'language', 'domain', 'source'],     \n",
    "               index_col='uri').filter(like='DH', axis=0)\n",
    "# load list of development files (and their domain)\n",
    "dev = read_csv(f'{ldc2019e31}/docs/sources.tbl', \n",
    "               delim_whitespace=True,\n",
    "               names=['uri', 'language', 'domain', 'source'], \n",
    "               index_col='uri').filter(like='DH', axis=0)\n",
    "\n",
    "# obtain list of domains\n",
    "domains = sorted(dev.domain.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create four files per (domain, subset) pair:\n",
    "- `{domain}.{subset}.txt` contains list of files\n",
    "- `{domain}.{subset.rttm` contains manual annotation\n",
    "- `{domain}.{subset}.uem` contains unpartitioned evaluation map (uem)\n",
    "- `{domain}.domain.{subset}.txt` contains file-to-domain mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from pyannote.database.util import load_rttm\n",
    "from pyannote.database.util import load_uem\n",
    "from pyannote.audio.features.utils import get_audio_duration\n",
    "from pyannote.core import Segment\n",
    "\n",
    "# split ldc2019e31 into training set (two third) and developement set (one third)\n",
    "\n",
    "# for each domain in ldc2019e31\n",
    "for domain, files in dev.groupby('domain'):\n",
    "    \n",
    "    # load unpartitioned evaluation map (uem)\n",
    "    uems = load_uem(f'{ldc2019e31}/data/single_channel/uem/{domain}.uem')\n",
    "    \n",
    "    # create four files per (domain, subset) pair\n",
    "    # {domain}.{subset}.txt contains list of files\n",
    "    # {domain}.{subset}.rttm contains manual annotation\n",
    "    # {domain}.{subset}.uem contains unpartitioned evaluation map (uem)\n",
    "    # {domain}.domain.{subset}.txt contains file-to-domain mapping\n",
    "    with open(f'{ROOT}/database/DIHARD/{domain}.dev.txt', 'w') as uris_dev, \\\n",
    "         open(f'{ROOT}/database/DIHARD/{domain}.trn.txt', 'w') as uris_trn, \\\n",
    "         open(f'{ROOT}/database/DIHARD/{domain}.dev.rttm', 'w') as rttm_dev, \\\n",
    "         open(f'{ROOT}/database/DIHARD/{domain}.trn.rttm', 'w') as rttm_trn, \\\n",
    "         open(f'{ROOT}/database/DIHARD/{domain}.dev.uem', 'w') as uem_dev, \\\n",
    "         open(f'{ROOT}/database/DIHARD/{domain}.trn.uem', 'w') as uem_trn, \\\n",
    "         open(f'{ROOT}/database/DIHARD/{domain}.domain.dev.txt', 'w') as domain_dev, \\\n",
    "         open(f'{ROOT}/database/DIHARD/{domain}.domain.trn.txt', 'w') as domain_trn:\n",
    "        \n",
    "        # for each file in current domain\n",
    "        for i, (uri, file) in enumerate(files.iterrows()):\n",
    "            \n",
    "            duration = get_audio_duration({'audio': f'{ROOT}/database/DIHARD/dev/{uri}.flac'})\n",
    "            # ugly hack to avoid rounding errors: this has the effect of not considering \n",
    "            # the last millisecond of each file\n",
    "            duration -= 0.001\n",
    "            support = Segment(0, duration)\n",
    "            \n",
    "            # i = 0 ==> dev\n",
    "            # i = 1 ==> trn\n",
    "            # i = 2 ==> trn\n",
    "            # i = 3 ==> dev\n",
    "            # i = 4 ==> trn\n",
    "            # i = 5 ==> trn\n",
    "            # i = 6 ==> dev \n",
    "            # ...\n",
    "            f_uris = uris_trn if i % 3 else uris_dev\n",
    "            f_uris.write(f'dev/{uri}\\n')\n",
    "            \n",
    "            # dump domain to disk\n",
    "            f_domain = domain_trn if i % 3 else domain_dev\n",
    "            f_domain.write(f'dev/{uri} {domain}\\n')\n",
    "            \n",
    "            # load and crop reference (cf above hack)\n",
    "            reference = load_rttm(f'{ldc2019e31}/data/single_channel/rttm/{uri}.rttm')[uri]\n",
    "            reference.uri = f'dev/{uri}'\n",
    "            reference = reference.crop(support, mode='intersection')\n",
    "            \n",
    "            # dump reference to disk\n",
    "            f_rttm = rttm_trn if i % 3 else rttm_dev\n",
    "            write_rttm(f_rttm, reference)\n",
    "            \n",
    "            # load and crop unpartitioned evaluation map\n",
    "            uem = uems[uri]\n",
    "            uem.uri = f'dev/{uri}'\n",
    "            uem = uem.crop(support, mode='intersection')\n",
    "            \n",
    "            # dump uem to disk\n",
    "            f_uem = uem_trn if i % 3 else uem_dev\n",
    "            write_uem(f_uem, uem)\n",
    "\n",
    "# same as above but applied to ldc2019e32 that is used entirely for test\n",
    "for domain, files in tst.groupby('domain'):\n",
    "    \n",
    "    uems = load_uem(f'{ldc2019e32}/data/single_channel/uem/{domain}.uem')\n",
    "\n",
    "    with open(f'{ROOT}/database/DIHARD/{domain}.tst.txt', 'w') as f_uris, \\\n",
    "         open(f'{ROOT}//database/DIHARD/{domain}.tst.rttm', 'w') as f_rttm, \\\n",
    "         open(f'{ROOT}/database/DIHARD/{domain}.tst.uem', 'w') as f_uem, \\\n",
    "         open(f'{ROOT}/database/DIHARD/{domain}.domain.tst.txt', 'w') as f_domain:\n",
    "\n",
    "        for i, (uri, file) in enumerate(files.iterrows()):\n",
    "            \n",
    "            duration = get_audio_duration({'audio': f'{ROOT}/database/DIHARD/tst/{uri}.flac'})\n",
    "            duration -= 0.001\n",
    "            support = Segment(0, duration)\n",
    "            \n",
    "            f_uris.write(f'tst/{uri}\\n')\n",
    "            \n",
    "            f_domain.write(f'tst/{uri} {domain}\\n')\n",
    "            \n",
    "            reference = load_rttm(f'{ldc2019e32}/data/single_channel/rttm/{uri}.rttm')[uri]\n",
    "            reference.uri = f'tst/{uri}'\n",
    "            reference = reference.crop(support, mode='intersection')\n",
    "\n",
    "            write_rttm(f_rttm, reference)\n",
    "            \n",
    "            uem = uems[uri]\n",
    "            uem.uri = f'tst/{uri}'\n",
    "            uem = uem.crop(support, mode='intersection')\n",
    "\n",
    "            write_uem(f_uem, uem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `database.yml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "database_yml = {\n",
    "    'Databases': {\n",
    "        'DIHARD': f'{ROOT}/database/DIHARD/{{uri}}.flac',\n",
    "        'MUSAN': f'{musan}/{{uri}}.wav',\n",
    "        'AMI': f'TODO/{{uri}}.wav',\n",
    "    },\n",
    "    'Protocols': {\n",
    "        'DIHARD': {'SpeakerDiarization': {}},\n",
    "        'AMI': {'SpeakerDiarization': 'TODO'},\n",
    "        'X': {'SpeakerDiarization': {}}\n",
    "    }\n",
    "}\n",
    "\n",
    "for domain in domains:\n",
    "    database_yml['Protocols']['DIHARD']['SpeakerDiarization'][f'{domain}'] = {}\n",
    "    for subset, short in {'train': 'trn', 'development': 'dev', 'test': 'tst'}.items():\n",
    "        database_yml['Protocols']['DIHARD']['SpeakerDiarization'][f'{domain}'][subset] = {\n",
    "            'uris': f'{ROOT}/database/DIHARD/{domain}.{short}.txt',\n",
    "            'annotation': f'{ROOT}/database/DIHARD/{domain}.{short}.rttm',\n",
    "            'annotated': f'{ROOT}/database/DIHARD/{domain}.{short}.uem',\n",
    "            'domain': f'{ROOT}/database/DIHARD/{domain}.domain.{short}.txt',\n",
    "        }\n",
    "    \n",
    "    all_but_domain = sorted(set(domains) - {domain})\n",
    "    database_yml['Protocols']['X']['SpeakerDiarization'][f'DIHARD_LeaveOneDomainOut_{domain}'] = {}\n",
    "    for subset in ['train', 'development']:\n",
    "        database_yml['Protocols']['X']['SpeakerDiarization'][f'DIHARD_LeaveOneDomainOut_{domain}'][subset] = {\n",
    "            f'DIHARD.SpeakerDiarization.{other_domain}': [subset] for other_domain in all_but_domain\n",
    "        }\n",
    "    database_yml['Protocols']['X']['SpeakerDiarization'][f'DIHARD_LeaveOneDomainOut_{domain}']['test'] = {\n",
    "        f'DIHARD.SpeakerDiarization.{domain}': ['test']\n",
    "    }   \n",
    "    \n",
    "database_yml['Protocols']['X']['SpeakerDiarization']['DIHARD_Official'] = {\n",
    "    subset: {\n",
    "        f'DIHARD.SpeakerDiarization.{domain}': [subset] for domain in domains\n",
    "    } for subset in ['train', 'development', 'test']\n",
    "}\n",
    "\n",
    "with open(f'{ROOT}/database.yml', 'w') as f:\n",
    "    f.write(yaml.dump(database_yml, \n",
    "                      default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `PYANNOTE_DATABASE_CONFIG` environment variable to `{ROOT}/database.yml` will give you a bunch of `pyannote.database` protocols:\n",
    "\n",
    "- `X.SpeakerDiarization.DIHARD_Official` is the official protocol for `DIHARD2` \n",
    "- `X.SpeakerDiarization.DIHARD_LeaveOneDomainOut_{domain}` uses all domains but {domain} in the training and development sets, and only {domain} in the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
